{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLINE OF FINAL REPORT?\n",
    "\n",
    "- Data\n",
    "- Sentiment Analysis\n",
    "- Character-level Text Generation\n",
    " - Maximum likelihood language model\n",
    " - LSTMs\n",
    " - Prediction methods (temperature, etc. -- could try beam search or ensemble methods)\n",
    "- Word-level Text Generation\n",
    " - (Embedding methods?)\n",
    " - Maximum likelihood language model\n",
    " - LSTMs\n",
    " - Prediction methods (temperature, etc.)\n",
    " - (Look at some form of variable importance for the final models?)\n",
    "- Conclusions and Future Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Catherine Li\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in STAT 110 for now; convert to lower case\n",
    "filename = \"cleaned_data/STAT/STAT 110.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple character-level maximum likelihood language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make sure to do plenty of cuties, most of the problems. this means you need to devote to this course with very difficult but very hard. but one of the toughest course i have take a high school. they aren't necessarily translate into the night to solve the problem sets! don't start the problem solving skills and a strong math background knowledge is invaluable.\n",
      "only take a pass/fail. i should have failed the course simply because of scheduling and it was really good compared to other math class will be easy). but if you don't do well in the park. still worth attending lectures. highly recommend it. but if you are interesting\n",
      "very useful material than any other introductory level course. then i heard that the course, but i also see changes.  that said, be forewarned that it's over, but i'd recommend this class is definitely try to attempt each problems as you can get through it, and it teaches you how to be a better understanding that he does.yeah, yeah, it will definitely, definitely ta\n"
     ]
    }
   ],
   "source": [
    "# Try a (not-so-)cute baseline; ML language model is pretty good, but bad grammar?\n",
    "# Code adapted from http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "\n",
    "from collections import *\n",
    "from random import random\n",
    "\n",
    "def train_char_lm(data, order = 4):    \n",
    "    # Initialize dictionary to hold sequences and their probable next letters\n",
    "    lm = defaultdict(Counter)\n",
    "    # Pad the data to start\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    \n",
    "    # Loop over every sequence in the corpus, tracking the letters that tend to appear after each sequence\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    # Normalize the counts into probabilities\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c, cnt/s) for c, cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "    # Get previous sequence for which we'll predict the next char\n",
    "    history = history[-order:] \n",
    "    # Get distribution of probable chars to follow\n",
    "    dist = lm[history]\n",
    "    # Roll the dice to generate next char with the probability given in the dist\n",
    "    x = random()\n",
    "    for c, v in dist:\n",
    "        x = x - v\n",
    "        if x <= 0: return c\n",
    "\n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    # Initialize with padding tildes\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    # Generate letters\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)\n",
    "\n",
    "lm = train_char_lm(raw_text, order = 10)\n",
    "print(generate_text(lm, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory recurrent neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  349982\n",
      "Total Vocab:  66\n",
      "Total Patterns:  349882\n"
     ]
    }
   ],
   "source": [
    "# Map unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# Reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# Normalize\n",
    "X = X / float(n_vocab)\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement some helpful generating functions\n",
    "# Includes a temperature parameter, which we'll play with later -- this changes how conservative the predictions are\n",
    "\n",
    "def sample(preds, temperature = 1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    # Code from https://stackoverflow.com/questions/37246030/how-to-change-the-temperature-of-a-softmax-output-in-keras\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.array(preds)**(1/temperature)\n",
    "    probas = np.random.multinomial(1, preds / preds.sum(), 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_from_lstm(dataX, model, num_chars = 1000, temperature = None):\n",
    "    # Pick a random seed\n",
    "    start = np.random.randint(0, len(dataX)-1)\n",
    "    pattern = dataX[start]\n",
    "    print(\"Seed:\")\n",
    "    print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "    # Generate characters\n",
    "    for i in range(num_chars):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(max(max(dataX)) + 1)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        if temperature == None:\n",
    "            index = np.argmax(prediction)\n",
    "        else:\n",
    "            index = sample(prediction[0], temperature = temperature)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        sys.stdout.write(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a 1-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the checkpoint\n",
    "filepath=\"char_keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-stat110-chars.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "349882/349882 [==============================] - 1743s 5ms/step - loss: 2.8459\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.84595, saving model to keras_checkpoints/weights-improvement-01-2.8459.hdf5\n",
      "Epoch 2/20\n",
      "349882/349882 [==============================] - 1760s 5ms/step - loss: 2.6077\n",
      "\n",
      "Epoch 00002: loss improved from 2.84595 to 2.60772, saving model to keras_checkpoints/weights-improvement-02-2.6077.hdf5\n",
      "Epoch 3/20\n",
      "349882/349882 [==============================] - 1734s 5ms/step - loss: 2.3955\n",
      "\n",
      "Epoch 00003: loss improved from 2.60772 to 2.39548, saving model to keras_checkpoints/weights-improvement-03-2.3955.hdf5\n",
      "Epoch 4/20\n",
      "349882/349882 [==============================] - 1740s 5ms/step - loss: 2.2513\n",
      "\n",
      "Epoch 00004: loss improved from 2.39548 to 2.25133, saving model to keras_checkpoints/weights-improvement-04-2.2513.hdf5\n",
      "Epoch 5/20\n",
      "349882/349882 [==============================] - 1550s 4ms/step - loss: 2.1472\n",
      "\n",
      "Epoch 00005: loss improved from 2.25133 to 2.14723, saving model to keras_checkpoints/weights-improvement-05-2.1472.hdf5\n",
      "Epoch 6/20\n",
      "349882/349882 [==============================] - 1426s 4ms/step - loss: 2.0689\n",
      "\n",
      "Epoch 00006: loss improved from 2.14723 to 2.06885, saving model to keras_checkpoints/weights-improvement-06-2.0689.hdf5\n",
      "Epoch 7/20\n",
      "349882/349882 [==============================] - 26888s 77ms/step - loss: 2.0085\n",
      "\n",
      "Epoch 00007: loss improved from 2.06885 to 2.00851, saving model to keras_checkpoints/weights-improvement-07-2.0085.hdf5\n",
      "Epoch 8/20\n",
      "349882/349882 [==============================] - 4710s 13ms/step - loss: 1.9574\n",
      "\n",
      "Epoch 00008: loss improved from 2.00851 to 1.95739, saving model to keras_checkpoints/weights-improvement-08-1.9574.hdf5\n",
      "Epoch 9/20\n",
      "349882/349882 [==============================] - 1877s 5ms/step - loss: 1.9142\n",
      "\n",
      "Epoch 00009: loss improved from 1.95739 to 1.91416, saving model to keras_checkpoints/weights-improvement-09-1.9142.hdf5\n",
      "Epoch 10/20\n",
      "349882/349882 [==============================] - 2501s 7ms/step - loss: 1.8754\n",
      "\n",
      "Epoch 00010: loss improved from 1.91416 to 1.87541, saving model to keras_checkpoints/weights-improvement-10-1.8754.hdf5\n",
      "Epoch 11/20\n",
      "349882/349882 [==============================] - 1867s 5ms/step - loss: 1.8434\n",
      "\n",
      "Epoch 00011: loss improved from 1.87541 to 1.84336, saving model to keras_checkpoints/weights-improvement-11-1.8434.hdf5\n",
      "Epoch 12/20\n",
      "349882/349882 [==============================] - 5853s 17ms/step - loss: 1.8133\n",
      "\n",
      "Epoch 00012: loss improved from 1.84336 to 1.81326, saving model to keras_checkpoints/weights-improvement-12-1.8133.hdf5\n",
      "Epoch 13/20\n",
      "349882/349882 [==============================] - 1798s 5ms/step - loss: 1.7876\n",
      "\n",
      "Epoch 00013: loss improved from 1.81326 to 1.78763, saving model to keras_checkpoints/weights-improvement-13-1.7876.hdf5\n",
      "Epoch 14/20\n",
      "349882/349882 [==============================] - 4175s 12ms/step - loss: 1.7650\n",
      "\n",
      "Epoch 00014: loss improved from 1.78763 to 1.76495, saving model to keras_checkpoints/weights-improvement-14-1.7650.hdf5\n",
      "Epoch 15/20\n",
      "349882/349882 [==============================] - 2437s 7ms/step - loss: 1.7410\n",
      "\n",
      "Epoch 00015: loss improved from 1.76495 to 1.74101, saving model to keras_checkpoints/weights-improvement-15-1.7410.hdf5\n",
      "Epoch 16/20\n",
      "349882/349882 [==============================] - 2904s 8ms/step - loss: 1.7229\n",
      "\n",
      "Epoch 00016: loss improved from 1.74101 to 1.72286, saving model to keras_checkpoints/weights-improvement-16-1.7229.hdf5\n",
      "Epoch 17/20\n",
      "349882/349882 [==============================] - 1597s 5ms/step - loss: 1.7058\n",
      "\n",
      "Epoch 00017: loss improved from 1.72286 to 1.70580, saving model to keras_checkpoints/weights-improvement-17-1.7058.hdf5\n",
      "Epoch 18/20\n",
      "349882/349882 [==============================] - 2031s 6ms/step - loss: 1.6905\n",
      "\n",
      "Epoch 00018: loss improved from 1.70580 to 1.69048, saving model to keras_checkpoints/weights-improvement-18-1.6905.hdf5\n",
      "Epoch 19/20\n",
      "349882/349882 [==============================] - 1453s 4ms/step - loss: 1.6747\n",
      "\n",
      "Epoch 00019: loss improved from 1.69048 to 1.67467, saving model to keras_checkpoints/weights-improvement-19-1.6747.hdf5\n",
      "Epoch 20/20\n",
      "349882/349882 [==============================] - 1450s 4ms/step - loss: 1.6601\n",
      "\n",
      "Epoch 00020: loss improved from 1.67467 to 1.66006, saving model to keras_checkpoints/weights-improvement-20-1.6601.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18268a2f4e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  do not take it. you're much better taking stat 104 or stat 139, where you will actually learn and r \"\n",
      "oe th the mrte taae the course an a sratt cnass and toeerstand the material in the course and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toeerstand the material in the coass and toee\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the network weights\n",
    "filename = \"char_keras_checkpoints/weights-improvement-20-1.6601.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "generate_from_lstm(dataX, model, num_chars = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a bigger model\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath=\"char_keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "349882/349882 [==============================] - 4968s 14ms/step - loss: 2.6992\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.69919, saving model to weights-improvement-01-2.6992-bigger.hdf5\n",
      "Epoch 2/20\n",
      "349882/349882 [==============================] - 4715s 13ms/step - loss: 2.1683\n",
      "\n",
      "Epoch 00002: loss improved from 2.69919 to 2.16834, saving model to weights-improvement-02-2.1683-bigger.hdf5\n",
      "Epoch 3/20\n",
      "349882/349882 [==============================] - 4741s 14ms/step - loss: 1.9108\n",
      "\n",
      "Epoch 00003: loss improved from 2.16834 to 1.91078, saving model to weights-improvement-03-1.9108-bigger.hdf5\n",
      "Epoch 4/20\n",
      "349882/349882 [==============================] - 4721s 13ms/step - loss: 1.7746\n",
      "\n",
      "Epoch 00004: loss improved from 1.91078 to 1.77462, saving model to weights-improvement-04-1.7746-bigger.hdf5\n",
      "Epoch 5/20\n",
      "349882/349882 [==============================] - 4713s 13ms/step - loss: 1.6846\n",
      "\n",
      "Epoch 00005: loss improved from 1.77462 to 1.68462, saving model to weights-improvement-05-1.6846-bigger.hdf5\n",
      "Epoch 6/20\n",
      "349882/349882 [==============================] - 4709s 13ms/step - loss: 1.6217\n",
      "\n",
      "Epoch 00006: loss improved from 1.68462 to 1.62170, saving model to weights-improvement-06-1.6217-bigger.hdf5\n",
      "Epoch 7/20\n",
      "349882/349882 [==============================] - 9435s 27ms/step - loss: 1.5722\n",
      "\n",
      "Epoch 00007: loss improved from 1.62170 to 1.57220, saving model to weights-improvement-07-1.5722-bigger.hdf5\n",
      "Epoch 8/20\n",
      "349882/349882 [==============================] - 8059s 23ms/step - loss: 1.5313\n",
      "\n",
      "Epoch 00008: loss improved from 1.57220 to 1.53127, saving model to weights-improvement-08-1.5313-bigger.hdf5\n",
      "Epoch 9/20\n",
      "349882/349882 [==============================] - 9178s 26ms/step - loss: 1.4990\n",
      "\n",
      "Epoch 00009: loss improved from 1.53127 to 1.49898, saving model to weights-improvement-09-1.4990-bigger.hdf5\n",
      "Epoch 10/20\n",
      "349882/349882 [==============================] - 9292s 27ms/step - loss: 1.4691\n",
      "\n",
      "Epoch 00010: loss improved from 1.49898 to 1.46915, saving model to weights-improvement-10-1.4691-bigger.hdf5\n",
      "Epoch 11/20\n",
      "349882/349882 [==============================] - 4807s 14ms/step - loss: 1.4454\n",
      "\n",
      "Epoch 00011: loss improved from 1.46915 to 1.44540, saving model to weights-improvement-11-1.4454-bigger.hdf5\n",
      "Epoch 12/20\n",
      "349882/349882 [==============================] - 4697s 13ms/step - loss: 1.4228\n",
      "\n",
      "Epoch 00012: loss improved from 1.44540 to 1.42276, saving model to weights-improvement-12-1.4228-bigger.hdf5\n",
      "Epoch 13/20\n",
      "349882/349882 [==============================] - 4692s 13ms/step - loss: 1.4027\n",
      "\n",
      "Epoch 00013: loss improved from 1.42276 to 1.40272, saving model to weights-improvement-13-1.4027-bigger.hdf5\n",
      "Epoch 14/20\n",
      "349882/349882 [==============================] - 4815s 14ms/step - loss: 1.3874\n",
      "\n",
      "Epoch 00014: loss improved from 1.40272 to 1.38743, saving model to weights-improvement-14-1.3874-bigger.hdf5\n",
      "Epoch 15/20\n",
      "349882/349882 [==============================] - 4729s 14ms/step - loss: 1.3698\n",
      "\n",
      "Epoch 00015: loss improved from 1.38743 to 1.36983, saving model to weights-improvement-15-1.3698-bigger.hdf5\n",
      "Epoch 16/20\n",
      "349882/349882 [==============================] - 4856s 14ms/step - loss: 1.3547\n",
      "\n",
      "Epoch 00016: loss improved from 1.36983 to 1.35465, saving model to weights-improvement-16-1.3547-bigger.hdf5\n",
      "Epoch 17/20\n",
      "349882/349882 [==============================] - 4715s 13ms/step - loss: 1.3418\n",
      "\n",
      "Epoch 00017: loss improved from 1.35465 to 1.34176, saving model to weights-improvement-17-1.3418-bigger.hdf5\n",
      "Epoch 18/20\n",
      "349882/349882 [==============================] - 4734s 14ms/step - loss: 1.3300\n",
      "\n",
      "Epoch 00018: loss improved from 1.34176 to 1.32999, saving model to weights-improvement-18-1.3300-bigger.hdf5\n",
      "Epoch 19/20\n",
      "349882/349882 [==============================] - 4720s 13ms/step - loss: 1.3202\n",
      "\n",
      "Epoch 00019: loss improved from 1.32999 to 1.32021, saving model to weights-improvement-19-1.3202-bigger.hdf5\n",
      "Epoch 20/20\n",
      "349882/349882 [==============================] - 4716s 13ms/step - loss: 1.3097\n",
      "\n",
      "Epoch 00020: loss improved from 1.32021 to 1.30967, saving model to weights-improvement-20-1.3097-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182044d0278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "model2.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" h had friday due dates. this made for a very challenging semester and some very late and stressful n \"\n",
      "athematical and the material is very difficult and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very interesting and the material is very int\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the network weights\n",
    "filename = \"char_keras_checkpoints/weights-improvement-20-1.3097-bigger.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "generate_from_lstm(dataX, model2, num_chars = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" tration requirement. it starts out easy but becomes surprisingly difficult, and the exams are harder \"\n",
      " than you the course that i have taken at harvard. this class is very interesting and the material is a great course that is alazing. but it is a great class that is a great course that is alazing the tes are gard and the problem sets are very interesting and the material is very hilarious and i don't take it if you want to do well in the eod of the course that is all about the class and the material is a great class. but it is a great class. but it is very dasefrlly and the most of the course is very hard and the material is very difficult and interesting and worth it if you are a lot of time in the work in the class is teaching the course is in the end of the course with the material and tecching statistics and the material is a great course that is interesting and the tes are interesting and the material is a great class. but it is a great course that i vould recommend this class in the course is is a freat class. but it is a great class that is a great class that you will learn the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Temperature?\n",
    "\n",
    "generate_from_lstm(dataX, model2, num_chars = 1000, temperature = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "349882/349882 [==============================] - 4779s 14ms/step - loss: 1.2608\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.26079, saving model to keras_checkpoints/weights-improvement-01-1.2608-bigger-batch512.hdf5\n",
      "Epoch 2/20\n",
      "349882/349882 [==============================] - 4844s 14ms/step - loss: 1.2461\n",
      "\n",
      "Epoch 00002: loss improved from 1.26079 to 1.24609, saving model to keras_checkpoints/weights-improvement-02-1.2461-bigger-batch512.hdf5\n",
      "Epoch 3/20\n",
      "349882/349882 [==============================] - 4828s 14ms/step - loss: 1.2370\n",
      "\n",
      "Epoch 00003: loss improved from 1.24609 to 1.23699, saving model to keras_checkpoints/weights-improvement-03-1.2370-bigger-batch512.hdf5\n",
      "Epoch 4/20\n",
      "349882/349882 [==============================] - 4829s 14ms/step - loss: 1.2314\n",
      "\n",
      "Epoch 00004: loss improved from 1.23699 to 1.23137, saving model to keras_checkpoints/weights-improvement-04-1.2314-bigger-batch512.hdf5\n",
      "Epoch 5/20\n",
      "349882/349882 [==============================] - 4816s 14ms/step - loss: 1.2228\n",
      "\n",
      "Epoch 00005: loss improved from 1.23137 to 1.22275, saving model to keras_checkpoints/weights-improvement-05-1.2228-bigger-batch512.hdf5\n",
      "Epoch 6/20\n",
      "349882/349882 [==============================] - 4844s 14ms/step - loss: 1.2199\n",
      "\n",
      "Epoch 00006: loss improved from 1.22275 to 1.21994, saving model to keras_checkpoints/weights-improvement-06-1.2199-bigger-batch512.hdf5\n",
      "Epoch 7/20\n",
      "349882/349882 [==============================] - 8139s 23ms/step - loss: 1.2133\n",
      "\n",
      "Epoch 00007: loss improved from 1.21994 to 1.21327, saving model to keras_checkpoints/weights-improvement-07-1.2133-bigger-batch512.hdf5\n",
      "Epoch 8/20\n",
      "349882/349882 [==============================] - 8925s 26ms/step - loss: 1.2081\n",
      "\n",
      "Epoch 00008: loss improved from 1.21327 to 1.20809, saving model to keras_checkpoints/weights-improvement-08-1.2081-bigger-batch512.hdf5\n",
      "Epoch 9/20\n",
      "349882/349882 [==============================] - 7309s 21ms/step - loss: 1.2022\n",
      "\n",
      "Epoch 00009: loss improved from 1.20809 to 1.20224, saving model to keras_checkpoints/weights-improvement-09-1.2022-bigger-batch512.hdf5\n",
      "Epoch 10/20\n",
      "349882/349882 [==============================] - 6894s 20ms/step - loss: 1.1984\n",
      "\n",
      "Epoch 00010: loss improved from 1.20224 to 1.19840, saving model to keras_checkpoints/weights-improvement-10-1.1984-bigger-batch512.hdf5\n",
      "Epoch 11/20\n",
      "349882/349882 [==============================] - 4981s 14ms/step - loss: 1.1940\n",
      "\n",
      "Epoch 00011: loss improved from 1.19840 to 1.19403, saving model to keras_checkpoints/weights-improvement-11-1.1940-bigger-batch512.hdf5\n",
      "Epoch 12/20\n",
      "349882/349882 [==============================] - 9049s 26ms/step - loss: 1.1914\n",
      "\n",
      "Epoch 00012: loss improved from 1.19403 to 1.19136, saving model to keras_checkpoints/weights-improvement-12-1.1914-bigger-batch512.hdf5\n",
      "Epoch 13/20\n",
      "349882/349882 [==============================] - 5563s 16ms/step - loss: 1.1867\n",
      "\n",
      "Epoch 00013: loss improved from 1.19136 to 1.18669, saving model to keras_checkpoints/weights-improvement-13-1.1867-bigger-batch512.hdf5\n",
      "Epoch 14/20\n",
      "349882/349882 [==============================] - 5367s 15ms/step - loss: 1.1813\n",
      "\n",
      "Epoch 00014: loss improved from 1.18669 to 1.18127, saving model to keras_checkpoints/weights-improvement-14-1.1813-bigger-batch512.hdf5\n",
      "Epoch 15/20\n",
      "349882/349882 [==============================] - 5065s 14ms/step - loss: 1.1772\n",
      "\n",
      "Epoch 00015: loss improved from 1.18127 to 1.17722, saving model to keras_checkpoints/weights-improvement-15-1.1772-bigger-batch512.hdf5\n",
      "Epoch 16/20\n",
      "349882/349882 [==============================] - 5001s 14ms/step - loss: 1.1745\n",
      "\n",
      "Epoch 00016: loss improved from 1.17722 to 1.17450, saving model to keras_checkpoints/weights-improvement-16-1.1745-bigger-batch512.hdf5\n",
      "Epoch 17/20\n",
      "349882/349882 [==============================] - 4984s 14ms/step - loss: 1.1705\n",
      "\n",
      "Epoch 00017: loss improved from 1.17450 to 1.17055, saving model to keras_checkpoints/weights-improvement-17-1.1705-bigger-batch512.hdf5\n",
      "Epoch 18/20\n",
      "349882/349882 [==============================] - 4975s 14ms/step - loss: 1.1684\n",
      "\n",
      "Epoch 00018: loss improved from 1.17055 to 1.16843, saving model to keras_checkpoints/weights-improvement-18-1.1684-bigger-batch512.hdf5\n",
      "Epoch 19/20\n",
      "349882/349882 [==============================] - 4971s 14ms/step - loss: 1.1638\n",
      "\n",
      "Epoch 00019: loss improved from 1.16843 to 1.16384, saving model to keras_checkpoints/weights-improvement-19-1.1638-bigger-batch512.hdf5\n",
      "Epoch 20/20\n",
      "349882/349882 [==============================] - 4977s 14ms/step - loss: 1.1602\n",
      "\n",
      "Epoch 00020: loss improved from 1.16384 to 1.16021, saving model to keras_checkpoints/weights-improvement-20-1.1602-bigger-batch512.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20791c7bb38>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size?\n",
    "\n",
    "filepath=\"char_keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-bigger-batch512.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model2.fit(X, y, epochs=20, batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  incredible amount of support (lots of sections, office hours, quora intuitive explanations, practic \"\n",
      "e problems and sections are very difficult and interesting and that she course is very difficult to start the world and the course is very hnteresting and terribly difficult and the class is very difficult and it is a great course that i vould recommend this class that i would recommend this class in the class in the concepts and make sure you are all the material in the class iard and teaching the course statt classes that i dad ciallenge your gand class that i dound the course seations are rea\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the network weights\n",
    "filename = \"char_keras_checkpoints/weights-improvement-20-1.1602-bigger-batch512.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "generate_from_lstm(dataX, model2, num_chars = 500, temperature = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger hidden layers? 2 layers, dim 512, and dropout 0.5\n",
    "# http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(512))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(y.shape[1], activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath = \"char_keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-bigger_dim512.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "349882/349882 [==============================] - 36335s 104ms/step - loss: 2.7210\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.72102, saving model to keras_checkpoints/weights-improvement-01-2.7210-bigger_dim512.hdf5\n",
      "Epoch 2/5\n",
      "349882/349882 [==============================] - 18045s 52ms/step - loss: 2.1393\n",
      "\n",
      "Epoch 00002: loss improved from 2.72102 to 2.13929, saving model to keras_checkpoints/weights-improvement-02-2.1393-bigger_dim512.hdf5\n",
      "Epoch 3/5\n",
      "349882/349882 [==============================] - 18327s 52ms/step - loss: 1.8474\n",
      "\n",
      "Epoch 00003: loss improved from 2.13929 to 1.84744, saving model to keras_checkpoints/weights-improvement-03-1.8474-bigger_dim512.hdf5\n",
      "Epoch 4/5\n",
      "349882/349882 [==============================] - 21339s 61ms/step - loss: 1.6943\n",
      "\n",
      "Epoch 00004: loss improved from 1.84744 to 1.69425, saving model to keras_checkpoints/weights-improvement-04-1.6943-bigger_dim512.hdf5\n",
      "Epoch 5/5\n",
      "349882/349882 [==============================] - 23051s 66ms/step - loss: 1.5992\n",
      "\n",
      "Epoch 00005: loss improved from 1.69425 to 1.59923, saving model to keras_checkpoints/weights-improvement-05-1.5992-bigger_dim512.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282b99c3518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit (only 5 epochs for the sake of time)\n",
    "model3.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "349882/349882 [==============================] - 17964s 51ms/step - loss: 1.5264\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.52643, saving model to keras_checkpoints/weights-improvement-01-2-1.5264-bigger_dim512.hdf5\n",
      "Epoch 2/15\n",
      "349882/349882 [==============================] - 17593s 50ms/step - loss: 1.4695\n",
      "\n",
      "Epoch 00002: loss improved from 1.52643 to 1.46950, saving model to keras_checkpoints/weights-improvement-02-2-1.4695-bigger_dim512.hdf5\n",
      "Epoch 3/15\n",
      "349882/349882 [==============================] - 32726s 94ms/step - loss: 1.4238\n",
      "\n",
      "Epoch 00003: loss improved from 1.46950 to 1.42379, saving model to keras_checkpoints/weights-improvement-03-2-1.4238-bigger_dim512.hdf5\n",
      "Epoch 4/15\n",
      "349882/349882 [==============================] - 17742s 51ms/step - loss: 1.3860\n",
      "\n",
      "Epoch 00004: loss improved from 1.42379 to 1.38595, saving model to keras_checkpoints/weights-improvement-04-2-1.3860-bigger_dim512.hdf5\n",
      "Epoch 5/15\n",
      "349882/349882 [==============================] - 17332s 50ms/step - loss: 1.3545\n",
      "\n",
      "Epoch 00005: loss improved from 1.38595 to 1.35453, saving model to keras_checkpoints/weights-improvement-05-2-1.3545-bigger_dim512.hdf5\n",
      "Epoch 6/15\n",
      "349882/349882 [==============================] - 17249s 49ms/step - loss: 1.3252\n",
      "\n",
      "Epoch 00006: loss improved from 1.35453 to 1.32524, saving model to keras_checkpoints/weights-improvement-06-2-1.3252-bigger_dim512.hdf5\n",
      "Epoch 7/15\n",
      "349882/349882 [==============================] - 17439s 50ms/step - loss: 1.2996\n",
      "\n",
      "Epoch 00007: loss improved from 1.32524 to 1.29963, saving model to keras_checkpoints/weights-improvement-07-2-1.2996-bigger_dim512.hdf5\n",
      "Epoch 8/15\n",
      "349882/349882 [==============================] - 17583s 50ms/step - loss: 1.2795\n",
      "\n",
      "Epoch 00008: loss improved from 1.29963 to 1.27955, saving model to keras_checkpoints/weights-improvement-08-2-1.2795-bigger_dim512.hdf5\n",
      "Epoch 9/15\n",
      "349882/349882 [==============================] - 17151s 49ms/step - loss: 1.2600\n",
      "\n",
      "Epoch 00009: loss improved from 1.27955 to 1.26003, saving model to keras_checkpoints/weights-improvement-09-2-1.2600-bigger_dim512.hdf5\n",
      "Epoch 10/15\n",
      "349882/349882 [==============================] - 17185s 49ms/step - loss: 1.2407\n",
      "\n",
      "Epoch 00010: loss improved from 1.26003 to 1.24067, saving model to keras_checkpoints/weights-improvement-10-2-1.2407-bigger_dim512.hdf5\n",
      "Epoch 11/15\n",
      "349882/349882 [==============================] - 17264s 49ms/step - loss: 1.2266\n",
      "\n",
      "Epoch 00011: loss improved from 1.24067 to 1.22657, saving model to keras_checkpoints/weights-improvement-11-2-1.2266-bigger_dim512.hdf5\n",
      "Epoch 12/15\n",
      "349882/349882 [==============================] - 17259s 49ms/step - loss: 1.2118\n",
      "\n",
      "Epoch 00012: loss improved from 1.22657 to 1.21184, saving model to keras_checkpoints/weights-improvement-12-2-1.2118-bigger_dim512.hdf5\n",
      "Epoch 13/15\n",
      "349882/349882 [==============================] - 18228s 52ms/step - loss: 1.2017\n",
      "\n",
      "Epoch 00013: loss improved from 1.21184 to 1.20173, saving model to keras_checkpoints/weights-improvement-13-2-1.2017-bigger_dim512.hdf5\n",
      "Epoch 14/15\n",
      "349882/349882 [==============================] - 17242s 49ms/step - loss: 1.1903\n",
      "\n",
      "Epoch 00014: loss improved from 1.20173 to 1.19034, saving model to keras_checkpoints/weights-improvement-14-2-1.1903-bigger_dim512.hdf5\n",
      "Epoch 15/15\n",
      "349882/349882 [==============================] - 17372s 50ms/step - loss: 1.1802\n",
      "\n",
      "Epoch 00015: loss improved from 1.19034 to 1.18022, saving model to keras_checkpoints/weights-improvement-15-2-1.1802-bigger_dim512.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2831f9b1b38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need more epochs\n",
    "# filename = \"char_keras_checkpoints/weights-improvement-05-1.5992-bigger_dim512.hdf5\"\n",
    "# model3.load_weights(filename)\n",
    "# model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath = \"char_keras_checkpoints/weights-improvement-{epoch:02d}-2-{loss:.4f}-bigger_dim512.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model3.fit(X, y, epochs=15, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" f thing, so if you have a busy schedule be prepared for an all-nighter almost every thursday. if you \"\n",
      " are willing to do well. but it is nnt a large amount of time in the class to anyone whoh a group of probability is and the tes are a great class to take concepts and think in a teal sequirg of the her a sections and office hours. it is iard to detote to this class to the problem sets and exams. it's a great class, but it is a great class, it's a great class, but it is a great class. bnd the tes are a great class. the material is very hnteresting and the material is interesting and the tes are a\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the network weights\n",
    "filename = \"char_keras_checkpoints/weights-improvement-15-2-1.1802-bigger_dim512.hdf5\"\n",
    "model3.load_weights(filename)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "generate_from_lstm(dataX, model3, num_chars = 500, temperature = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs. epoch for the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search?\n",
    "# https://github.com/karpathy/char-rnn/issues/138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUs? Sequence length?\n",
    "# https://stackoverflow.com/questions/47125723/keras-lstm-for-text-generation-keeps-repeating-a-line-or-a-sequence/48430652#48430652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembling?\n",
    "# https://arxiv.org/pdf/1704.00109.pdf\n",
    "\n",
    "# Other background / params\n",
    "# https://cs.stanford.edu/~zxie/textgen.pdf\n",
    "# https://arxiv.org/pdf/1707.05589.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
